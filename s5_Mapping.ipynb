{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19747d6b-a99e-4a36-a02a-1d8f6693cb99",
   "metadata": {},
   "source": [
    "# Concept Mapping and Generation of PSEVs for Each Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11e8cc-df82-461f-90ef-583d0c85a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# display and widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# strings\n",
    "import re\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "import requests\n",
    "from typing import Dict, List, Set\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "import statistics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbe1b5-acbb-42ce-a3d8-f32346030832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the base_path to the IC data location in Wynton\n",
    "\n",
    "\n",
    "# Functions for easy pulling of CDW data\n",
    "\n",
    "def file_path_parquet(filename, datatype):\n",
    "    base_path = f\"path/to/ic/data/{datatype}/\"\n",
    "    parquet_wild = \"/*.parquet\"\n",
    "    return f\"{base_path}{filename}{parquet_wild}\"\n",
    "\n",
    "def rtime():\n",
    "    # Get the current datetime\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Define a mapping of days of the week to colors\n",
    "    day_color_mapping = {\n",
    "        0: 'red',       # Monday\n",
    "        1: 'orange',    # Tuesday\n",
    "        2: 'green',     # Wednesday\n",
    "        3: 'blue',      # Thursday\n",
    "        4: 'purple',    # Friday\n",
    "        5: 'brown',     # Saturday\n",
    "        6: 'gray',      # Sunday\n",
    "    }\n",
    "\n",
    "    # Get the day of the week (0=Monday, 1=Tuesday, ..., 6=Sunday)\n",
    "    day_of_week = current_datetime.weekday()\n",
    "    # Get the color based on the day of the week\n",
    "    text_color = day_color_mapping.get(day_of_week, 'black')  # Default to black if the day is not found in the mapping\n",
    "    # Format the current datetime\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Generate the formatted output with the corresponding color\n",
    "    formatted_output = f\"\\n<b><span style='color:{text_color}'>Ran: {formatted_datetime}</span></b>\\n\"\n",
    "    # Display the formatted output using Markdown\n",
    "    display(Markdown(formatted_output))\n",
    "    \n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88b203-c352-4b49-982c-936b47df462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the path to scratch and the username\n",
    "\n",
    "\n",
    "# wynton_username with your actual Wynton username\n",
    "username = 'name'\n",
    "\n",
    "# Spill data that doesn't fit into memory into Wynton Scratch storage (BeeGFS)\n",
    "# Increase up to 12 threads and 150 GB of memory to not overwhelm the system\n",
    "# Recommendation: ~12 GB of memory for each thread\n",
    "# reduce if there are other system limitations in place\n",
    "config_query = f\"\"\"\n",
    "    SET temp_directory = 'path/to/scratch/{username}/duckdb_dir';\n",
    "    SET preserve_insertion_order = false;\n",
    "    SET memory_limit = '150GB';\n",
    "    SET threads TO 12;\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection with configurations\n",
    "con = duckdb.connect()\n",
    "con_info = con.execute(config_query)  # Apply configuration settings\n",
    "\n",
    "display(con_info)\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9cd7c5-3d12-4725-a49c-6c00ec36260d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642797f0-5946-48d8-bf61-f5637379815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! load whichever is relevant\n",
    "\n",
    "# p_cohort = pd.read_parquet(\"p1_cohort.parquet\")\n",
    "\n",
    "# p_cohort = pd.read_parquet(\"p3_cohort.parquet\")\n",
    "\n",
    "# p_cohort = pd.read_parquet(\"p5_cohort.parquet\")\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6262bab-7940-4cf0-b140-2f8ea2a07c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p_cohort.copy()\n",
    "person_id_index = df['patientepicid'].to_list()\n",
    "df.drop('patientepicid', axis=1, inplace=True)\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5b36f-fc2d-4a4b-9ff6-09bb1fac653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('is_ms', axis=1)\n",
    "y = df['is_ms']\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348c213-3c3f-4ce8-b1f4-7956c1dca411",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuis = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019be0c-bc58-41f2-9994-a517ae6e71d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90383f10-2284-4939-b1a3-a30483e6400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BioPortalMapper (BPM) from CUI to any other ontology selected in the BioPortal interface\n",
    "class BMP_from_CUI:\n",
    "    def __init__(self, api_key: str, max_concurrent: int = 10, monitor_performance: bool = True):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"http://data.bioontology.org\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"apikey token={api_key}\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.response_times = []\n",
    "        self.monitor_performance = monitor_performance\n",
    "        \n",
    "    def extract_id_from_url(self, url: str, ontology: str) -> str:\n",
    "        \"\"\"Extract the actual ID from the BioPortal URL\"\"\"\n",
    "        return url.split('/')[-1] if url else \"\"\n",
    "\n",
    "    def get_mappings_for_cui(self, cui: str) -> Dict:\n",
    "        \"\"\"Get mappings for a single CUI\"\"\"\n",
    "        mappings = defaultdict(set)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{self.base_url}/search\",\n",
    "                headers=self.headers,\n",
    "                params={\n",
    "                    \"q\": cui,\n",
    "                    \"require_exact_match\": \"false\",\n",
    "                    \"pagesize\": 50,\n",
    "                    \"include\": \"prefLabel,cui\",\n",
    "                    \"display_context\": \"false\",\n",
    "                    \"display_links\": \"true\"\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if self.monitor_performance:\n",
    "                self.response_times.append(time.time() - start_time)\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            for result in data.get(\"collection\", []):\n",
    "                links = result.get(\"links\", {})\n",
    "                ont_url = links.get(\"ontology\", \"\")\n",
    "                ont_acronym = ont_url.split(\"/\")[-1] if ont_url else \"\"\n",
    "                \n",
    "                class_url = result.get(\"@id\", \"\")\n",
    "                pref_label = result.get(\"prefLabel\", \"\")\n",
    "                cui_list = result.get(\"cui\", [])\n",
    "                \n",
    "                class_id = self.extract_id_from_url(class_url, ont_acronym)\n",
    "                \n",
    "                if ont_acronym and class_id and cui in cui_list:\n",
    "                    mappings[ont_acronym].add((class_id, pref_label))\n",
    "                    \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing CUI {cui}: {str(e)}\")\n",
    "            return {}\n",
    "            \n",
    "        return {k: {\"terms\": [{\"id\": id, \"label\": label} for id, label in v]} \n",
    "                for k, v in mappings.items()}\n",
    "\n",
    "    def process_cui_batch(self, cuis: List[str]) -> Dict[str, Dict]:\n",
    "        \"\"\"Process a batch of CUIs\"\"\"\n",
    "        results = {}\n",
    "        for cui in cuis:\n",
    "            results[cui] = self.get_mappings_for_cui(cui)\n",
    "            time.sleep(0.07)  # rate limit (<15 requests/second)\n",
    "        return results\n",
    "\n",
    "    def batch_process_cuis(self, cui_list: List[str], batch_size: int = 100) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Process CUIs in batches using concurrent processing\n",
    "        \"\"\"\n",
    "        all_mappings = {}\n",
    "        total_batches = (len(cui_list) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Split CUIs into batches\n",
    "        cui_batches = [cui_list[i:i + batch_size] for i in range(0, len(cui_list), batch_size)]\n",
    "        \n",
    "        print(f\"\\nProcessing {len(cui_list)} CUIs in {total_batches} batches\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:\n",
    "\n",
    "            future_to_batch = {\n",
    "                executor.submit(self.process_cui_batch, batch): i \n",
    "                for i, batch in enumerate(cui_batches)\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=len(cui_batches), desc=\"Processing batches\") as pbar:\n",
    "                for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                    batch_num = future_to_batch[future]\n",
    "                    try:\n",
    "                        batch_results = future.result()\n",
    "                        all_mappings.update(batch_results)\n",
    "                        \n",
    "                        if self.monitor_performance and self.response_times:\n",
    "                            avg_response = statistics.mean(self.response_times[-100:])\n",
    "                            pbar.set_postfix({\n",
    "                                'avg_response': f'{avg_response:.2f}s',\n",
    "                                'batch': batch_num\n",
    "                            })\n",
    "                            \n",
    "                            if avg_response > 2.0: \n",
    "                                self.max_concurrent = max(1, self.max_concurrent - 1)\n",
    "                            elif avg_response < 1.0:\n",
    "                                self.max_concurrent = min(15, self.max_concurrent + 1)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError processing batch {batch_num}: {str(e)}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        return all_mappings\n",
    "    \n",
    "    \n",
    "\n",
    "# BioPortalMapper (BPM) from SPOKE embeddings to CUIs, creates a reversible\n",
    "class BPM_SPOKE_to_CUI:\n",
    "    def __init__(self, api_key: str, max_concurrent: int = 10, monitor_performance: bool = True):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"http://data.bioontology.org\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"apikey token={api_key}\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.response_times = []\n",
    "        self.monitor_performance = monitor_performance\n",
    "\n",
    "    def get_cuis(self, term: str) -> List[str]:\n",
    "        \"\"\"Get all CUIs for a given term\"\"\"\n",
    "        start_time = time.time()\n",
    "        cuis = set() \n",
    "        \n",
    "        try:\n",
    "            # if CUI, return it\n",
    "            if term.startswith('C') and term[1:].isdigit():\n",
    "                cuis.add(term)\n",
    "            \n",
    "            # If DOID, get CUIs from database_cross_reference\n",
    "            elif term.startswith('DOID:'):\n",
    "                doid_num = term.replace('DOID:', '')\n",
    "                response = requests.get(\n",
    "                    f\"{self.base_url}/ontologies/DOID/classes/http%3A%2F%2Fpurl.obolibrary.org%2Fobo%2FDOID_{doid_num}\",\n",
    "                    headers=self.headers,\n",
    "                    params={\"include\": \"properties\"}\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    if 'properties' in data:\n",
    "                        # Get CUIs from hasDbXref\n",
    "                        xrefs = data['properties'].get('http://www.geneontology.org/formats/oboInOwl#hasDbXref', [])\n",
    "                        for ref in xrefs:\n",
    "                            if ref.startswith('UMLS_CUI:'):\n",
    "                                cuis.add(ref.replace('UMLS_CUI:', ''))\n",
    "            \n",
    "            # For all terms (or if no CUIs found yet), try search endpoint\n",
    "            if not cuis or not term.startswith('DOID:'):\n",
    "                response = requests.get(\n",
    "                    f\"{self.base_url}/search\",\n",
    "                    headers=self.headers,\n",
    "                    params={\n",
    "                        \"q\": term,\n",
    "                        \"require_exact_match\": \"true\",\n",
    "                        \"include\": \"cui\",\n",
    "                        \"display_context\": \"false\",\n",
    "                        \"pagesize\": 100,\n",
    "                        \"ontologies\": \"UMLS\"  # target UMLS ontology\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    for result in data.get(\"collection\", []):\n",
    "                        if \"cui\" in result and isinstance(result[\"cui\"], list):\n",
    "                            cuis.update(result[\"cui\"])\n",
    "                        elif \"cui\" in result and isinstance(result[\"cui\"], str):\n",
    "                            cuis.add(result[\"cui\"])\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error processing term {term}: {str(e)}\")\n",
    "        finally:\n",
    "            if self.monitor_performance:\n",
    "                self.response_times.append(time.time() - start_time)\n",
    "\n",
    "        return list(cuis)\n",
    "\n",
    "    def process_cui_batch(self, terms: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Process a batch of terms and return their CUIs\"\"\"\n",
    "        results = {}\n",
    "        for term in terms:\n",
    "            cuis = self.get_cuis(term)\n",
    "            if cuis:\n",
    "                results[term] = cuis\n",
    "            time.sleep(0.07)  # rate limit\n",
    "        return results\n",
    "\n",
    "    def batch_process_cuis(self, term_list: List[str], batch_size: int = 100) -> Dict[str, List[str]]:\n",
    "        \"\"\"Process terms in batches using concurrent processing\"\"\"\n",
    "        all_mappings = {}\n",
    "        total_batches = (len(term_list) + batch_size - 1) // batch_size\n",
    "        \n",
    "        term_batches = [term_list[i:i + batch_size] for i in range(0, len(term_list), batch_size)]\n",
    "        \n",
    "        print(f\"\\nProcessing {len(term_list)} terms in {total_batches} batches\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:\n",
    "            future_to_batch = {\n",
    "                executor.submit(self.process_cui_batch, batch): i \n",
    "                for i, batch in enumerate(term_batches)\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=len(term_batches), desc=\"Processing batches\") as pbar:\n",
    "                for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                    batch_num = future_to_batch[future]\n",
    "                    try:\n",
    "                        batch_results = future.result()\n",
    "                        all_mappings.update(batch_results)\n",
    "                        \n",
    "                        if self.monitor_performance and self.response_times:\n",
    "                            avg_response = statistics.mean(self.response_times[-100:])\n",
    "                            pbar.set_postfix({\n",
    "                                'avg_response': f'{avg_response:.2f}s',\n",
    "                                'batch': batch_num\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\nError processing batch {batch_num}: {str(e)}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        return all_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea14eb-da2f-4f1a-8cc4-8c093b604f2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Mapping AWAY from CUI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1c06b-18bd-417c-bd11-a53be51705d0",
   "metadata": {},
   "source": [
    "Maps AWAY from CUIs to the designated ontologies in BioPortal. This can be set by the user in the web interface to return faster response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe5583-a291-477c-bfd3-2cf55552a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"your-API-key\"\n",
    "\n",
    "mapper = BMP_from_CUI(API_KEY, max_concurrent=5, monitor_performance=True)\n",
    "\n",
    "mappings = mapper.batch_process_cuis(cuis, batch_size=20)\n",
    "\n",
    "print(f\"\\nProcessed {len(mappings)} CUIs\")\n",
    "print(f\"Average response time: {statistics.mean(mapper.response_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca25731-ac36-4738-9c14-8dce15e0d80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary to a JSON file\n",
    "with open('mappings.json', 'w') as json_file:\n",
    "    json.dump(mappings, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c94b5-51e8-436b-b06f-153b7f8f129b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Mapping TO CUI from SPOKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c69f3-0d4e-4b62-9e70-db8c2b8394cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spoke = np.load('PSEV_matrix')\n",
    "sep = np.load('PSEV_SEP_map')\n",
    "spoke_node = np.load('PSEV_SPOKE_node_map')\n",
    "\n",
    "spoke = pd.DataFrame(spoke, columns=spoke_node)\n",
    "spoke.index = sep\n",
    "spoke.index = spoke.index.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "spoke.columns = spoke.columns.map(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246917bd-9416-4908-bac5-57f229bc2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices\n",
    "spoke_ind = list(spoke.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197ac96-5e00-4786-a713-c48e1466af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"your-API-key\"\n",
    "\n",
    "mapper = BPM_SPOKE_to_CUI(API_KEY, max_concurrent=5, monitor_performance=True)\n",
    "\n",
    "spoke_ind_mini = spoke_ind\n",
    "\n",
    "spoke_mappings = mapper.batch_process_cuis(spoke_ind_mini, batch_size=5)\n",
    "\n",
    "print(f\"\\nProcessed {len(spoke_mappings)} CUIs\")\n",
    "print(f\"Average response time: {statistics.mean(mapper.response_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d1b13-62d9-49dc-b8e8-23e90c669e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary to a JSON file\n",
    "with open('spoke_mappings.json', 'w') as json_file:\n",
    "    json.dump(spoke_mappings, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df6be2-89a5-481e-ab04-43a992a79c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spoke_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba785d75-95eb-48df-80c4-cc17ea4171a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768d68f-2018-4bea-b3b1-ffef20c4ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reverse the dictionary\n",
    "def reverse_dict(original):\n",
    "    reversed_dict = {}\n",
    "    for key, values in original.items():\n",
    "        for value in values:\n",
    "            reversed_dict[value] = key\n",
    "    return reversed_dict\n",
    "\n",
    "# Function to combine columns and rename them based on the mapping\n",
    "def combine_columns(df, mapping):\n",
    "    # First filter to only keep columns that are in the mapping\n",
    "    valid_columns = [col for col in df.columns if col in mapping]\n",
    "    df_filtered = df[valid_columns]\n",
    "    \n",
    "    # Group columns by their mapped values\n",
    "    column_groups = defaultdict(list)\n",
    "    for col in df_filtered.columns:\n",
    "        column_groups[mapping[col]].append(col)\n",
    "    \n",
    "    # Create all combined columns at once\n",
    "    combined_cols = {\n",
    "        new_col: df_filtered[old_cols].sum(axis=1) \n",
    "        for new_col, old_cols in column_groups.items()\n",
    "    }\n",
    "    \n",
    "    # Create new dataframe all at once\n",
    "    new_df = pd.DataFrame(combined_cols)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "# Reverse the dictionary\n",
    "spoke_mappings_rev = reverse_dict(spoke_mappings)\n",
    "print(\"Reversed spoke mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278b6f10-985f-4bf3-b6f3-22c3fc71866b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! Apply the transformation\n",
    "#! This is important to accurately merge the CUI data with SPOKE data\n",
    "\n",
    "X_rev = combine_columns(X, spoke_mappings_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d649158-6a6d-45f6-8f3d-6a555bb46bc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64414a2-b119-47e8-9b49-aab0d557dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_dot_product(df, matrix, num_chunks):\n",
    "    results = []\n",
    "    chunk_size = (df.shape[0] + num_chunks - 1) // num_chunks  # Calculate chunk size based on num_chunks\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, df.shape[0])\n",
    "        \n",
    "        chunk = df.iloc[start_idx:end_idx]\n",
    "        result_chunk = np.dot(chunk.values, matrix.values)\n",
    "        results.append(result_chunk)\n",
    "    \n",
    "    return np.vstack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd44440-f592-4498-a62d-160e87223d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Remember to load the X for the proper cohort above in the `Data` section\n",
    "#! Then, obtain the combined version of the data in the `Mapping TO CUI from SPOKE` section to get X_rev\n",
    "\n",
    "concept_list = list(X_rev.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f264c-51d8-440b-aa33-ea516d68eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove SEPs that are not in our cohort\n",
    "spoke_filt = spoke.loc[spoke.index.isin(concept_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63bec8-23ba-4ff6-ac14-2734c8776ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSEVs for each patient\n",
    "patient_psevs = np.dot(X.values, spoke_filt.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5198679-6be6-4069-ab1b-82e1b9b4e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_psevs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccef427-fd67-447d-9013-682e31209891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! save the data\n",
    "#! do this for each cohort!\n",
    "\n",
    "np.save('p_cohort.npy', patient_psevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c872076-9f26-4f8d-9fe8-90124a0a599b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Segment SPOKE top 30% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db73a5b-2d06-4102-84b5-884a77467203",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = np.load('node_type_list.npy')\n",
    "node_type = [x.decode('utf-8') if isinstance(x, bytes) else x for x in node_type]\n",
    "node_type = pd.DataFrame({\n",
    "    'node': spoke.columns,\n",
    "    'type': node_type\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a9598-f1a4-42f8-a52d-83ad630817b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_node_types = node_type['type'].unique()\n",
    "\n",
    "for nt in unique_node_types:\n",
    "    nt_patient_psevs = patient_psevs[:, node_type[node_type['type'] == nt].index]\n",
    "    nodes = node_type[node_type['type'] == nt]['node']\n",
    "\n",
    "    # Step 1: Calculate variance for each column\n",
    "    chunk_size = 1000\n",
    "    num_columns = nt_patient_psevs.shape[1]\n",
    "    variances = []\n",
    "\n",
    "    for start in range(0, num_columns, chunk_size):\n",
    "        end = min(start + chunk_size, num_columns)\n",
    "        chunk = nt_patient_psevs[:, start:end]\n",
    "        chunk_variances = np.var(chunk, axis=0)\n",
    "        variances.extend(chunk_variances)\n",
    "\n",
    "    variances = np.array(variances)\n",
    "\n",
    "    # Step 2: Determine the threshold for the top 30%\n",
    "    threshold = np.percentile(variances, 70)  # 70th percentile\n",
    "\n",
    "    # Step 3: Find the columns with variance above the threshold\n",
    "    selected_columns = variances > threshold\n",
    "\n",
    "    # Step 4: Filter the array to retain only these columns\n",
    "    filtered_array = nt_patient_psevs[:, selected_columns]\n",
    "    nodes = np.array(nodes[selected_columns])\n",
    "\n",
    "    #Save the node-specific file\n",
    "    np.save(f'p5/filtered_patient_psevs_{nt}.npy', filtered_array)\n",
    "    np.save(f'p5/filtered_patient_psevs_columns_{nt}.npy', nodes)\n",
    "\n",
    "np.save('p5/person_id_index.npy', np.array(person_id_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149478a2-564c-4f9b-9141-4bfd29f1f50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16581c1a-a701-41ff-afb2-b496dc25c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select = model_eval_metrics(X, y, logreg, select=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e74856-82c4-4e14-bb42-d5ae8762f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select_scale = model_eval_metrics(X, y, logreg, select=True, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01937666-850a-48a2-a16b-fe5bd118a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! be sure to load in the p_cohort that has the new SPOKE mappings and make this the new X and y\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select_scale_spoke = model_eval_metrics(X, y, logreg, select=True, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92feca-778d-428a-9a3c-9ac24273bae4",
   "metadata": {},
   "source": [
    "**SPOKE Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848b1f5-63b1-410e-a20b-002ccbbb63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_psev_matrices(directory):\n",
    "    # Get all files that start with 'filtered_patient_psevs_' but don't include 'columns'\n",
    "    psev_files = [f for f in os.listdir(directory) \n",
    "                  if f.startswith('filtered_patient_psevs_') \n",
    "                  and 'columns' not in f\n",
    "                  and f != 'person_id_index.npy']\n",
    "    \n",
    "    matrices = []\n",
    "    column_names = []\n",
    "    \n",
    "    print(\"Loading matrices:\")\n",
    "    for psev_file in sorted(psev_files):\n",
    "        matrix = np.load(os.path.join(directory, psev_file))\n",
    "        \n",
    "        # Load corresponding column names\n",
    "        col_file = psev_file.replace('filtered_patient_psevs_', \n",
    "                                   'filtered_patient_psevs_columns_')\n",
    "        cols = np.load(os.path.join(directory, col_file), allow_pickle=True)  # Added allow_pickle=True\n",
    "        \n",
    "        print(f\"{psev_file}: shape {matrix.shape}, {len(cols)} columns\")\n",
    "        \n",
    "        matrices.append(matrix)\n",
    "        column_names.extend(cols)\n",
    "    \n",
    "    # concatenate horizontally\n",
    "    combined_matrix = np.hstack(matrices)\n",
    "    \n",
    "    # load person ids\n",
    "    person_ids = np.load(os.path.join(directory, 'person_id_index.npy'), allow_pickle=True)  # Added allow_pickle=True\n",
    "    \n",
    "    print(f\"\\nFinal matrix shape: {combined_matrix.shape}\")\n",
    "    print(f\"Number of columns: {len(column_names)}\")\n",
    "    print(f\"Number of patients: {len(person_ids)}\")\n",
    "    \n",
    "    return combined_matrix, column_names, person_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369704ac-9de8-4cf8-a526-3507811019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/psev/p5'\n",
    "psev_mat, psev_cols, psev_pats = load_psev_matrices(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823b673-3357-4d82-8461-267d5e633aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! be sure to load in the p_cohort that has the new SPOKE mappings and make this the new X and y\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select_scale_spoke = model_eval_metrics(psev_mat, y, logreg, select=True, scale=True, psev_in=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ce059-5b43-408b-8d83-40aa42c6ef81",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7b5a0-23e8-4fe4-b086-774406bc68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_95_ci(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    n = len(data)\n",
    "    \n",
    "    # For 95% CI, we use 1.96 as the critical value\n",
    "    margin_of_error = 1.96 * (std / np.sqrt(n))\n",
    "    \n",
    "    ci_lower = mean - margin_of_error\n",
    "    ci_upper = mean + margin_of_error\n",
    "    \n",
    "    return (ci_lower, ci_upper)\n",
    "\n",
    "\n",
    "# ci = calculate_95_ci(roc_scores)\n",
    "# print(f\"95% CI: ({ci[0]:.4f}, {ci[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601ef02-98d6-4b6c-9d7d-d936d4dc46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a figure of all the AUC ROCs\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "\n",
    "\n",
    "#! Below are all the lines for different y_pred_proba for each model\n",
    "\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, no_select_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#4B8BCB', lw=2.5,  \n",
    "#          label=f'All CUIs (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, no_scale_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#EED91F', lw=2.5,  \n",
    "#          label=f'Select CUIs (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, no_spoke_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#FF8C42', lw=2.5,  \n",
    "#          label=f'SPOKE CUIs (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, psev_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#FF3C38', lw=2.5,  \n",
    "#          label=f'PSEV (AUC = {np.mean(roc_scores):.4f})')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='#7B7D7D', linestyle='--', lw=1.5,  \n",
    "         label='Random (AUC = 0.5)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=14, labelpad=10)\n",
    "plt.ylabel('True Positive Rate', fontsize=14, labelpad=10)\n",
    "plt.title('Model Performance: ROC Curve', fontsize=16, pad=20)\n",
    "plt.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omop-query",
   "language": "python",
   "name": "omop-query"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
