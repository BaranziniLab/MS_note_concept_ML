{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d4c0e31-c525-4360-9f36-41f3e1d6d744",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Propensity Score and Other Matching Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363e451-a7dd-4a41-9b4d-2720754ce4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# timeoutput\n",
    "import datetime\n",
    "\n",
    "# regex\n",
    "import re\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd211c70-63d1-41ec-8f42-8b8aaca1deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the base_path to the IC data location in Wynton\n",
    "\n",
    "\n",
    "# Functions for easy pulling of CDW data\n",
    "\n",
    "def file_path_parquet(filename, datatype):\n",
    "    base_path = f\"path/to/ic/data/{datatype}/\"\n",
    "    parquet_wild = \"/*.parquet\"\n",
    "    return f\"{base_path}{filename}{parquet_wild}\"\n",
    "\n",
    "def rtime():\n",
    "    # Get the current datetime\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Define a mapping of days of the week to colors\n",
    "    day_color_mapping = {\n",
    "        0: 'red',       # Monday\n",
    "        1: 'orange',    # Tuesday\n",
    "        2: 'green',     # Wednesday\n",
    "        3: 'blue',      # Thursday\n",
    "        4: 'purple',    # Friday\n",
    "        5: 'brown',     # Saturday\n",
    "        6: 'gray',      # Sunday\n",
    "    }\n",
    "\n",
    "    # Get the day of the week (0=Monday, 1=Tuesday, ..., 6=Sunday)\n",
    "    day_of_week = current_datetime.weekday()\n",
    "    # Get the color based on the day of the week\n",
    "    text_color = day_color_mapping.get(day_of_week, 'black')  # Default to black if the day is not found in the mapping\n",
    "    # Format the current datetime\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Generate the formatted output with the corresponding color\n",
    "    formatted_output = f\"\\n<b><span style='color:{text_color}'>Ran: {formatted_datetime}</span></b>\\n\"\n",
    "    # Display the formatted output using Markdown\n",
    "    display(Markdown(formatted_output))\n",
    "    \n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf8164-970c-43c2-b037-568449e6927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the path to scratch and the username\n",
    "\n",
    "\n",
    "# wynton_username with your actual Wynton username\n",
    "username = 'name'\n",
    "\n",
    "# Spill data that doesn't fit into memory into Wynton Scratch storage (BeeGFS)\n",
    "# Increase up to 12 threads and 150 GB of memory to not overwhelm the system\n",
    "# Recommendation: ~12 GB of memory for each thread\n",
    "# reduce if there are other system limitations in place\n",
    "config_query = f\"\"\"\n",
    "    SET temp_directory = 'path/to/scratch/{username}/duckdb_dir';\n",
    "    SET preserve_insertion_order = false;\n",
    "    SET memory_limit = '150GB';\n",
    "    SET threads TO 12;\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection with configurations\n",
    "con = duckdb.connect()\n",
    "con_info = con.execute(config_query)  # Apply configuration settings\n",
    "\n",
    "display(con_info)\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d1b2f-ca9c-4b95-b357-47bffd49fb90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b0ba9-14ff-4426-9a06-bb3c8169d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_list = [374919, 4178929, 4145049, 4137855, 37110514]\n",
    "\n",
    "bad_pats = ['-1', '*Unspecified']\n",
    "\n",
    "ms_pats = pd.read_csv(\"data/ms_cohort_250318.csv\")\n",
    "ctl_pats = con.read_parquet(\"data/control_pats_250318.parquet\").df()\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afceb6-02da-4879-b964-cd9e1694dc34",
   "metadata": {},
   "source": [
    "### OMOP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43449a24-41d9-4faa-88d3-f7a0abf76374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition_occurrence\n",
    "condition_occurrence_ucsf = con.read_parquet(file_path_parquet('condition_occurrence', 'DEID_OMOP'))\n",
    "\n",
    "# person demographics\n",
    "person_ucsf = con.read_parquet(file_path_parquet('person', 'DEID_OMOP'))\n",
    "\n",
    "# person linkage OMOP - CDW\n",
    "person_extension_ucsf = con.read_parquet(file_path_parquet('person_extension', 'DEID_OMOP'))\n",
    "\n",
    "# visit_occurrence\n",
    "visit_occurrence_ucsf = con.read_parquet(file_path_parquet('visit_occurrence', 'DEID_OMOP'))\n",
    "\n",
    "# condition occurrence to link to CDW\n",
    "condition_occurrence_extension_ucsf = con.read_parquet(file_path_parquet('condition_occurrence_extension', 'DEID_OMOP'))\n",
    "\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76a34c-0de1-4ba3-88de-f4c33406549c",
   "metadata": {},
   "source": [
    "### CDW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b5a82-e36b-40cc-8543-a21787cfcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deid_note_key and negation terms\n",
    "note_concepts = con.read_parquet(file_path_parquet('note_concepts', 'DEID_CDW'))\n",
    "\n",
    "# linker to patientdurablekey, encoutnerkey, and deid_note_key\n",
    "note_metadata = con.read_parquet(file_path_parquet('note_metadata', 'DEID_CDW'))\n",
    "\n",
    "# note text - only deid_note_key and note_text\n",
    "note_text = con.read_parquet(file_path_parquet('note_text', 'DEID_CDW'))\n",
    "\n",
    "# diagnosis event fact\n",
    "diag_fact = con.read_parquet(file_path_parquet('diagnosiseventfact', 'DEID_CDW'))\n",
    "\n",
    "# patdurabledim\n",
    "patdurabledim = con.read_parquet(file_path_parquet('patdurabledim', 'DEID_CDW'))\n",
    "\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e3a7-4f1d-4968-9421-f16980e10bf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Demographic Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0076f-2431-4e22-b92e-9b8f39690ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ms_demo = f\"\"\"\n",
    "SELECT ms.patientepicid,\n",
    "    ms.note_fdate,\n",
    "    ms.preex_years AS follow_up,\n",
    "    ms.preex_note_count AS n_notes,\n",
    "    CAST(DATEDIFF('day', \n",
    "        CAST(prsn.birth_datetime AS DATE), \n",
    "        CAST(ms.note_fdate AS DATE)\n",
    "    ) / 365.25 AS INT) AS age_at_first_visit,\n",
    "    gender_concept_id,\n",
    "    race_concept_id,\n",
    "    1 AS is_ms\n",
    "FROM ms_pats ms\n",
    "JOIN (\n",
    "    SELECT person_id,\n",
    "        birth_datetime,\n",
    "        gender_concept_id,\n",
    "        race_concept_id\n",
    "    FROM person_ucsf\n",
    ") prsn ON ms.person_id = prsn.person_id\n",
    "WHERE age_at_first_visit >= 0\n",
    "    AND follow_up >= 1\n",
    "\"\"\"\n",
    "\n",
    "# optional keep note_fdate\n",
    "ms_demo = con.query(query_ms_demo).df().drop('note_fdate', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a82b2-d1b6-46d0-b40d-02a542c072ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ctl_demo = f\"\"\"\n",
    "SELECT con.patientepicid,\n",
    "    con.note_fdate,\n",
    "    con.note_years AS follow_up,\n",
    "    con.note_count AS n_notes,\n",
    "    CAST(DATEDIFF('day', \n",
    "        CAST(prsn.birth_datetime AS DATE), \n",
    "        CAST(con.note_fdate AS DATE)\n",
    "    ) / 365.25 AS INT) AS age_at_first_visit,\n",
    "    gender_concept_id,\n",
    "    race_concept_id,\n",
    "    0 AS is_ms\n",
    "FROM ctl_pats con\n",
    "JOIN (\n",
    "    SELECT person_id,\n",
    "        birth_datetime,\n",
    "        gender_concept_id,\n",
    "        race_concept_id\n",
    "    FROM person_ucsf\n",
    ") prsn ON con.person_id = prsn.person_id\n",
    "WHERE age_at_first_visit >= 0\n",
    "    AND follow_up >= 1\n",
    "\"\"\"\n",
    "\n",
    "# optional keep note_fdate\n",
    "ctl_demo = con.query(query_ctl_demo).df().drop('note_fdate', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bbef1-642b-4da1-8a7e-fe153572de54",
   "metadata": {},
   "source": [
    "**race_concept_id**\n",
    "<br>8515: Asian\n",
    "<br>8516, Black or African American \n",
    "<br>8522, Other\n",
    "<br>8527: White\n",
    "<br>8552: None, Declined, Unknown, Unknown/Declined\n",
    "<br>8557: Native Hawaiian, Native Hawaiian or Other Pacific Islander, Other Pacific Islander\n",
    "<br>8657: Native American or Alaska Native\n",
    "\n",
    "\n",
    "**gender_concept_id**\n",
    "<br>8507: Male\n",
    "<br>8521: Nonbinary\n",
    "<br>8532: Female\n",
    "<br>8551: Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83e5951-0963-4768-a3df-c003dace9b02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Propensity and Other Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6616182-2155-45c7-bb98-3d4d741f7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_demo = pd.concat([ctl_demo, ms_demo]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbda7b-b88c-4049-886a-8d387009cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optional to clear up memory\n",
    "# del ctl_demo, ms_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecab544-c231-418b-a7a3-53bb4a486734",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd50fc5-f563-427c-8274-a725ce4589a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy to modify this version\n",
    "psm_data = concat_demo.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5615bf-7ccf-4973-a722-15dff5c4e206",
   "metadata": {
    "tags": []
   },
   "source": [
    "## sklearn Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688adb38-a00a-4909-8cff-1c3e5a73e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_propensity_scores(df, treatment_col='is_ms'):\n",
    "    numeric_features = ['follow_up', 'age_at_first_visit', 'n_notes']\n",
    "    categorical_features = ['gender_concept_id', 'race_concept_id']\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    X = df[numeric_features + categorical_features]\n",
    "    y = df[treatment_col]\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "    logistic = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "    logistic.fit(X_transformed, y)\n",
    "    propensity_scores = logistic.predict_proba(X_transformed)[:, 1]\n",
    "    \n",
    "    return propensity_scores, X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9fce6-a4e7-4732-9920-971596bc876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very simple matching\n",
    "def perform_matching(df, treatment_col, propensity_scores, n_neighbors=1):\n",
    "    treated = df[df[treatment_col] == 1]\n",
    "    control = df[df[treatment_col] == 0]\n",
    "    \n",
    "    control_scores = propensity_scores[control.index]\n",
    "    treated_scores = propensity_scores[treated.index]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nbrs.fit(control_scores.reshape(-1, 1))\n",
    "    distances, indices = nbrs.kneighbors(treated_scores.reshape(-1, 1))\n",
    "\n",
    "    matched_control_indices = control.iloc[indices.flatten()].index\n",
    "    matched_treated = treated\n",
    "    matched_control = df.loc[matched_control_indices]\n",
    "\n",
    "    matched_data = pd.concat([matched_treated, matched_control])\n",
    "    \n",
    "    return matched_data\n",
    "\n",
    "\n",
    "# Simple caliper matching across some columns.\n",
    "# No additional coding/handling of vars, so not for multivariate use\n",
    "def perform_caliper_matching(df, treatment_col, propensity_scores, caliper=0.05, n_neighbors=1):\n",
    "    treated = df[df[treatment_col] == 1]\n",
    "    control = df[df[treatment_col] == 0]\n",
    "    treated_scores = propensity_scores[treated.index]\n",
    "    control_scores = propensity_scores[control.index]\n",
    "\n",
    "    nbrs = NearestNeighbors(radius=caliper, n_neighbors=n_neighbors)\n",
    "    nbrs.fit(control_scores.reshape(-1, 1))\n",
    "    \n",
    "    distances, indices = nbrs.radius_neighbors(treated_scores.reshape(-1, 1))\n",
    "    \n",
    "    matched_control_indices = []\n",
    "    for ind in indices:\n",
    "        if len(ind) > 0:\n",
    "            # select the closest n_neighbors matches (or fewer if less are found)\n",
    "            matched_control_indices.extend(control.iloc[ind[:n_neighbors]].index.tolist())\n",
    "    \n",
    "    matched_control = df.loc[matched_control_indices]\n",
    "    matched_treated = treated.loc[np.repeat(treated.index, n_neighbors)[:len(matched_control_indices)]]\n",
    "    \n",
    "    matched_data = pd.concat([matched_treated, matched_control])\n",
    "    return matched_data\n",
    "\n",
    "\n",
    "\n",
    "# This method works very well\n",
    "def multi_covariate_adjusted_matching(df, treatment_col, propensity_scores, caliper=0.05, n_neighbors=1, numeric_method='scaler', bin_features=None):\n",
    "    numeric_features = ['follow_up', 'age_at_first_visit', 'n_notes']\n",
    "    categorical_features = ['gender_concept_id', 'race_concept_id']\n",
    "    \n",
    "    # Handle binning\n",
    "    if numeric_method in ['binned', 'binned_scaler'] and bin_features:\n",
    "        def bin_age_to_decade(age):\n",
    "            return (age - 1) // 10 + 1\n",
    "        for feature in bin_features:\n",
    "            df[feature] = df[feature].apply(bin_age_to_decade)\n",
    "    \n",
    "    # One-hot encode categorical variables before scaling to avoid misinterpretation of encoded data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    df_transformed = preprocessor.fit_transform(df)\n",
    "\n",
    "    # Handle scaling after encoding\n",
    "    if numeric_method in ['scaler', 'binned_scaler']:\n",
    "        scaler = StandardScaler()\n",
    "        # Assuming numerical data are the first len(numeric_features) columns in df_transformed\n",
    "        df_transformed[:, :len(numeric_features)] = scaler.fit_transform(df_transformed[:, :len(numeric_features)])\n",
    "    else:\n",
    "        raise ValueError(\"Numeric method not valid/complete\")\n",
    "    \n",
    "    # Combine transformed DataFrame with propensity scores\n",
    "    propensity_scores = propensity_scores.reshape(-1, 1)\n",
    "    df_transformed = np.hstack([propensity_scores, df_transformed])\n",
    "    \n",
    "    treated_indices = df.index[df[treatment_col] == 1].tolist()\n",
    "    control_indices = df.index[df[treatment_col] == 0].tolist()\n",
    "    \n",
    "    treated_transformed = df_transformed[treated_indices]\n",
    "    control_transformed = df_transformed[control_indices]\n",
    "    \n",
    "    # NearestNeighbors\n",
    "    nbrs = NearestNeighbors(radius=caliper, n_neighbors=n_neighbors, metric='euclidean')\n",
    "    nbrs.fit(control_transformed)\n",
    "    \n",
    "    distances, indices = nbrs.radius_neighbors(treated_transformed)\n",
    "    \n",
    "    matched_control_indices = []\n",
    "    matched_treated_indices = []\n",
    "    used_control_indices = set()  # keeps track of used controls\n",
    "    \n",
    "    for i, ind in enumerate(indices):\n",
    "        if len(ind) > 0:\n",
    "            # filter out already used controls\n",
    "            available_controls = [idx for idx in ind if control_indices[idx] not in used_control_indices]\n",
    "            \n",
    "            if available_controls:\n",
    "                # closest n_neighbors matches from available controls\n",
    "                n_to_match = min(n_neighbors, len(available_controls))\n",
    "                closest_n = available_controls[:n_to_match]\n",
    "                \n",
    "                new_control_indices = [control_indices[j] for j in closest_n]\n",
    "                matched_control_indices.extend(new_control_indices)\n",
    "                matched_treated_indices.extend([treated_indices[i]])\n",
    "                \n",
    "                # Mark these controls as used\n",
    "                used_control_indices.update(new_control_indices)\n",
    "    \n",
    "    matched_control = df.loc[matched_control_indices].copy()\n",
    "    matched_treated = df.loc[matched_treated_indices].copy()\n",
    "    \n",
    "    # Add a matching group identifier\n",
    "    matched_treated['match_group'] = range(len(matched_treated))\n",
    "    matched_control['match_group'] = np.repeat(range(len(matched_treated)), \n",
    "                                             n_neighbors)[:len(matched_control)]\n",
    "    \n",
    "    # Combine matched treated and control groups\n",
    "    matched_data = pd.concat([matched_treated, matched_control])\n",
    "    \n",
    "    return matched_data\n",
    "\n",
    "\n",
    "\n",
    "# This method works and is optional to use.\n",
    "# You might get better matches depending on the selection criteria\n",
    "def mahalanobis_matching(df, treatment_col, caliper=0.05, n_neighbors=1, numeric_method='scaler', bin_features=None):\n",
    "    numeric_features = ['follow_up', 'age_at_first_visit', 'n_notes']\n",
    "    categorical_features = ['gender_concept_id', 'race_concept_id']\n",
    "    \n",
    "    # Create a copy and handle binning\n",
    "    df = df.copy()\n",
    "    if numeric_method in ['binned', 'binned_scaler'] and bin_features:\n",
    "        def bin_age_to_decade(age):\n",
    "            return (age - 1) // 10 + 1\n",
    "        for feature in bin_features:\n",
    "            df[feature] = df[feature].apply(bin_age_to_decade)\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numeric_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Transform data\n",
    "    df_transformed = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # Handle scaling\n",
    "    if numeric_method in ['scaler', 'binned_scaler']:\n",
    "        scaler = StandardScaler()\n",
    "        df_transformed[:, :len(numeric_features)] = scaler.fit_transform(df_transformed[:, :len(numeric_features)])\n",
    "    else:\n",
    "        raise ValueError(\"Numeric method not valid/complete\")\n",
    "    \n",
    "    # Split into treated and control\n",
    "    treated_mask = df[treatment_col] == 1\n",
    "    control_mask = ~treated_mask\n",
    "    \n",
    "    treated_indices = df.index[treated_mask].tolist()\n",
    "    control_indices = df.index[control_mask].tolist()\n",
    "    \n",
    "    treated_transformed = df_transformed[treated_mask]\n",
    "    control_transformed = df_transformed[control_mask]\n",
    "    \n",
    "    # Calculate Mahalanobis distances using vectorized operations\n",
    "    cov = EmpiricalCovariance().fit(df_transformed)\n",
    "    distances = cdist(treated_transformed, control_transformed, \n",
    "                     metric='mahalanobis', VI=cov.precision_)\n",
    "    \n",
    "    # Find matches within caliper\n",
    "    matched_pairs = []  # Store (treated_idx, control_idx) pairs\n",
    "    used_control_indices = set()\n",
    "    \n",
    "    for i in range(len(treated_indices)):\n",
    "        valid_matches = np.where(distances[i] <= caliper)[0]\n",
    "        available_matches = [j for j in valid_matches if control_indices[j] not in used_control_indices]\n",
    "        \n",
    "        if available_matches:\n",
    "            n_to_match = min(n_neighbors, len(available_matches))\n",
    "            closest = np.argsort(distances[i][available_matches])[:n_to_match]\n",
    "            selected_matches = [available_matches[j] for j in closest]\n",
    "            \n",
    "            # Store the pairs of indices\n",
    "            for control_idx in [control_indices[j] for j in selected_matches]:\n",
    "                matched_pairs.append((treated_indices[i], control_idx))\n",
    "            used_control_indices.update(control_indices[j] for j in selected_matches)\n",
    "    \n",
    "    # Separate treated and control indices from pairs\n",
    "    matched_treated_indices = [pair[0] for pair in matched_pairs]\n",
    "    matched_control_indices = [pair[1] for pair in matched_pairs]\n",
    "    \n",
    "    # Get unique treated indices while preserving order\n",
    "    unique_treated_indices = list(dict.fromkeys(matched_treated_indices))\n",
    "    \n",
    "    # Create matched dataset\n",
    "    matched_treated = df.loc[unique_treated_indices].copy()\n",
    "    matched_control = df.loc[matched_control_indices].copy()\n",
    "    \n",
    "    # Add matching group identifier\n",
    "    n_groups = len(unique_treated_indices)\n",
    "    group_map = {idx: i for i, idx in enumerate(unique_treated_indices)}\n",
    "    \n",
    "    matched_treated['match_group'] = range(n_groups)\n",
    "    matched_control['match_group'] = [group_map[treated_idx] for treated_idx in matched_treated_indices]\n",
    "    \n",
    "    # Combine matched pairs\n",
    "    matched_data = pd.concat([matched_treated, matched_control])\n",
    "    \n",
    "    return matched_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22e065-22f1-42a1-a88f-175c101b0ace",
   "metadata": {},
   "source": [
    "### Propensity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30888137-19b6-4837-b254-bc0cb21944f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_col = 'is_ms'\n",
    "\n",
    "# Calculate propensity scores\n",
    "propensity_scores, X_transformed = calculate_propensity_scores(psm_data, treatment_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232fa11-92b1-4daa-a41a-82422dc23aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the propensity score support\n",
    "psm_data['propensity_score'] = propensity_scores\n",
    "\n",
    "sns.kdeplot(data=psm_data, x='propensity_score', hue='is_ms')\n",
    "plt.title('Density Plot of Propensity Scores by Treatment Status')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "# Cumulative Distribution Function (CDF)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.ecdfplot(data=psm_data, x='propensity_score', hue='is_ms')\n",
    "plt.title('CDF of Propensity Scores by Treatment Status')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f743e523-10dc-44cb-9c29-6c2cf518d35d",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c01837-8cc1-41ee-a9a1-8a0483aab355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset this for safety\n",
    "psm_data = concat_demo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a812df-28a9-4e9c-b312-af416d2cd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform matching\n",
    "matched_data = multi_covariate_adjusted_matching(psm_data, treatment_col, \n",
    "                                                 propensity_scores, caliper=0.1, \n",
    "                                                 n_neighbors=25, numeric_method='scaler', \n",
    "                                                 bin_features=['age_at_first_visit'])\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7829f-7b8b-41a5-8d0c-645d7c144482",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74edfd-5957-47a9-a19e-2adf16d22279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After matching, you can verify the results:\n",
    "def verify_matching_results(matched_data, treatment_col):\n",
    "    print(\"Matching Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Count unique patients in each group\n",
    "    n_treated = matched_data[matched_data[treatment_col] == 1]['patientepicid'].nunique()\n",
    "    n_control = matched_data[matched_data[treatment_col] == 0]['patientepicid'].nunique()\n",
    "    \n",
    "    print(f\"Unique treated patients: {n_treated}\")\n",
    "    print(f\"Unique control patients: {n_control}\")\n",
    "    print(f\"Matching ratio (control:treated): {n_control/n_treated:.2f}:1\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    treated_dups = matched_data[matched_data[treatment_col] == 1]['patientepicid'].duplicated().sum()\n",
    "    control_dups = matched_data[matched_data[treatment_col] == 0]['patientepicid'].duplicated().sum()\n",
    "    \n",
    "    print(f\"\\nDuplicate treated patients: {treated_dups}\")\n",
    "    print(f\"Duplicate control patients: {control_dups}\")\n",
    "    \n",
    "    return matched_data.drop_duplicates(subset=['patientepicid', treatment_col])\n",
    "\n",
    "clean_matched_data = verify_matching_results(matched_data, 'is_ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da219b34-8a60-4407-b988-3c95a4e1b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "def summary_stats_table(data, covariates, categorical, treatment_col):\n",
    "    \n",
    "    def round4(num):\n",
    "        round(num, 3)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    # numeric covariates\n",
    "    for covariate in covariates:\n",
    "        mean_treated = data[data[treatment_col] == 1][covariate].mean()\n",
    "        std_treated = data[data[treatment_col] == 1][covariate].std()\n",
    "        mean_control = data[data[treatment_col] == 0][covariate].mean()\n",
    "        std_control = data[data[treatment_col] == 0][covariate].std()\n",
    "\n",
    "        smd = calculate_smd(data[data[treatment_col] == 1], \n",
    "                            data[data[treatment_col] == 0], \n",
    "                            [covariate])[covariate]\n",
    "\n",
    "        rows.append({\n",
    "            'Covariate': covariate,\n",
    "            'Mean_Treated': mean_treated,\n",
    "            'Std_Treated': std_treated,\n",
    "            'Mean_Control': mean_control,\n",
    "            'Std_Control': std_control,\n",
    "            'SMD': smd\n",
    "        })\n",
    "    \n",
    "    # categorical covariates\n",
    "    for covariate in categorical:\n",
    "        prop_treated = data[data[treatment_col] == 1][covariate].value_counts(normalize=True)\n",
    "        prop_control = data[data[treatment_col] == 0][covariate].value_counts(normalize=True)\n",
    "        \n",
    "        smds = calculate_smd_categorical(prop_treated, prop_control, covariate)\n",
    "        \n",
    "        for category in prop_treated.index:\n",
    "            rows.append({\n",
    "                'Covariate': f\"{covariate}_{category}\",\n",
    "                'Mean_Treated': prop_treated[category] * 100,\n",
    "                'Std_Treated': np.nan,  # Standard deviation is not applicable for proportions\n",
    "                'Mean_Control': prop_control[category] * 100,\n",
    "                'Std_Control': np.nan,  # Standard deviation is not applicable for proportions\n",
    "                'SMD': smds[category]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def calculate_smd(group1, group2, var_list):\n",
    "    smds = {}\n",
    "    for var in var_list:\n",
    "        mean1 = group1[var].mean()\n",
    "        mean2 = group2[var].mean()\n",
    "        std1 = group1[var].std()\n",
    "        std2 = group2[var].std()\n",
    "\n",
    "        smd = abs(mean1 - mean2) / np.sqrt((std1**2 + std2**2) / 2)\n",
    "        smds[var] = smd\n",
    "    return smds\n",
    "\n",
    "\n",
    "def calculate_smd_categorical(prop_treated, prop_control, covariate):\n",
    "    smds = {}\n",
    "    for category in prop_treated.index:\n",
    "        prop_treated_val = prop_treated.get(category, 0)\n",
    "        prop_control_val = prop_control.get(category, 0)\n",
    "        \n",
    "        smd = abs(prop_treated_val - prop_control_val) / np.sqrt((prop_treated_val * (1 - prop_treated_val) + prop_control_val * (1 - prop_control_val)) / 2)\n",
    "        smds[category] = smd\n",
    "    return smds\n",
    "\n",
    "\n",
    "def plot_covariate_balance(data, covariates, treatment_col):\n",
    "    for covariate in covariates:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.kdeplot(data[data[treatment_col] == 1][covariate], label='Treated')\n",
    "        sns.kdeplot(data[data[treatment_col] == 0][covariate], label='Control')\n",
    "        plt.title(f'Distribution of {covariate} by Treatment Status')\n",
    "        plt.xlabel(covariate)\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95dd209-57ca-4885-9f2c-ab4aaee46cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = ['follow_up', 'n_notes', 'age_at_first_visit']\n",
    "categorical = ['gender_concept_id', 'race_concept_id']   # numerics that should be treated as categoricals\n",
    "summary_stats = summary_stats_table(matched_data, covariates, categorical, 'is_ms')\n",
    "np.round(summary_stats, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6dda1a-e33b-406f-af85-0373f6d95616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE\n",
    "plot_covariate_balance(matched_data, covariates, 'is_ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a5a1a-f36a-4374-b524-c23fe7fa064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots\n",
    "for covariate in covariates:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.boxplot(x='is_ms', y=covariate, data=matched_data)\n",
    "    plt.title(f'Box Plot of {covariate} by Treatment Status')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f94faa-e930-4511-8e87-32a407ea3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate specific covariates\n",
    "col_to_plot = 'age_at_first_visit'\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=psm_data, x=col_to_plot, hue='is_ms', element='step', stat='density', common_norm=False, log_scale=10)\n",
    "plt.title('Before Matching')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=matched_data, x=col_to_plot, hue='is_ms', element='step', stat='density', common_norm=False, log_scale=10)\n",
    "plt.title('After Matching')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a5a71-8ae0-43a3-a454-8c7ff9bd1e27",
   "metadata": {},
   "source": [
    "### Save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40a38f-72c3-4289-90ac-cab5c173d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data.to_csv(\"matched25_cohort.csv\", index=False)\n",
    "\n",
    "rtime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omop-query",
   "language": "python",
   "name": "omop-query"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
