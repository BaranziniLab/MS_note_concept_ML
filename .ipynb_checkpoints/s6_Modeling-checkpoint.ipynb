{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4553464-c047-432a-852a-3228c29fb711",
   "metadata": {},
   "source": [
    "# Modeling Each Cohort and Model Variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11e8cc-df82-461f-90ef-583d0c85a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# display and widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# strings\n",
    "import re\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data\n",
    "from collections import Counter\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbe1b5-acbb-42ce-a3d8-f32346030832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the base_path to the IC data location in Wynton\n",
    "\n",
    "\n",
    "# Functions for easy pulling of CDW data\n",
    "\n",
    "def file_path_parquet(filename, datatype):\n",
    "    base_path = f\"path/to/ic/data/{datatype}/\"\n",
    "    parquet_wild = \"/*.parquet\"\n",
    "    return f\"{base_path}{filename}{parquet_wild}\"\n",
    "\n",
    "def rtime():\n",
    "    # Get the current datetime\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Define a mapping of days of the week to colors\n",
    "    day_color_mapping = {\n",
    "        0: 'red',       # Monday\n",
    "        1: 'orange',    # Tuesday\n",
    "        2: 'green',     # Wednesday\n",
    "        3: 'blue',      # Thursday\n",
    "        4: 'purple',    # Friday\n",
    "        5: 'brown',     # Saturday\n",
    "        6: 'gray',      # Sunday\n",
    "    }\n",
    "\n",
    "    # Get the day of the week (0=Monday, 1=Tuesday, ..., 6=Sunday)\n",
    "    day_of_week = current_datetime.weekday()\n",
    "    # Get the color based on the day of the week\n",
    "    text_color = day_color_mapping.get(day_of_week, 'black')  # Default to black if the day is not found in the mapping\n",
    "    # Format the current datetime\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Generate the formatted output with the corresponding color\n",
    "    formatted_output = f\"\\n<b><span style='color:{text_color}'>Ran: {formatted_datetime}</span></b>\\n\"\n",
    "    # Display the formatted output using Markdown\n",
    "    display(Markdown(formatted_output))\n",
    "    \n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88b203-c352-4b49-982c-936b47df462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the path to scratch and the username\n",
    "\n",
    "\n",
    "# wynton_username with your actual Wynton username\n",
    "username = 'name'\n",
    "\n",
    "# Spill data that doesn't fit into memory into Wynton Scratch storage (BeeGFS)\n",
    "# Increase up to 12 threads and 150 GB of memory to not overwhelm the system\n",
    "# Recommendation: ~12 GB of memory for each thread\n",
    "# reduce if there are other system limitations in place\n",
    "config_query = f\"\"\"\n",
    "    SET temp_directory = 'path/to/scratch/{username}/duckdb_dir';\n",
    "    SET preserve_insertion_order = false;\n",
    "    SET memory_limit = '150GB';\n",
    "    SET threads TO 12;\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection with configurations\n",
    "con = duckdb.connect()\n",
    "con_info = con.execute(config_query)  # Apply configuration settings\n",
    "\n",
    "display(con_info)\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9cd7c5-3d12-4725-a49c-6c00ec36260d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642797f0-5946-48d8-bf61-f5637379815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! load whichever is relevant\n",
    "\n",
    "# p_cohort = pd.read_parquet(\"p1_cohort.parquet\")\n",
    "# p_cohort = pd.read_parquet(\"p5_cohort_spoke.parquet\")\n",
    "\n",
    "# p_cohort = pd.read_parquet(\"p3_cohort.parquet\")\n",
    "# p_cohort = pd.read_parquet(\"p5_cohort_spoke.parquet\")\n",
    "\n",
    "# p_cohort = pd.read_parquet(\"p5_cohort.parquet\")\n",
    "# p_cohort = pd.read_parquet(\"p5_cohort_spoke.parquet\")\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6262bab-7940-4cf0-b140-2f8ea2a07c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p_cohort.copy()\n",
    "person_id_index = df['patientepicid'].to_list()\n",
    "df.drop('patientepicid', axis=1, inplace=True)\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de5b36f-fc2d-4a4b-9ff6-09bb1fac653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('is_ms', axis=1)\n",
    "y = df['is_ms']\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348c213-3c3f-4ce8-b1f4-7956c1dca411",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuis = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019be0c-bc58-41f2-9994-a517ae6e71d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d968e-c1e9-4c1c-b840-f36813badbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval_metrics(X, y, model, scale=False, select=False, psev_in=False):\n",
    "    # Store original column names\n",
    "    original_columns = X.columns if hasattr(X, 'columns') else None\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale if requested\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        print(\"Data scaled\")\n",
    "        \n",
    "    # Feature selection\n",
    "    if select:\n",
    "        pre_selector = RandomForestClassifier(n_estimators=1000, random_state=42, n_jobs=40)\n",
    "        selector = SelectFromModel(pre_selector, prefit=False, max_features=2000)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "        if not psev_in:\n",
    "            selected_features_mask = selector.get_support()\n",
    "            selected_feature_names = original_columns[selected_features_mask]\n",
    "        if psev_in:\n",
    "            selected_features_mask = selector.get_support()\n",
    "            selected_feature_names = [col for idx, col in enumerate(columns) if selected_features_mask[idx]]\n",
    "        print(\"Features selected\")\n",
    "    \n",
    "    # Determine feature names for importance analysis\n",
    "    if select or psev_in:\n",
    "        feature_names = selected_feature_names\n",
    "    elif original_columns is not None:\n",
    "        feature_names = original_columns\n",
    "    else:\n",
    "        print(\"The data does not have the correct format to interpret coefficients\")\n",
    "        return None\n",
    "    \n",
    "    # Fit model and make predictions\n",
    "    print(\"Fitting model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model fit\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'coef_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.coef_[0]\n",
    "        })\n",
    "        feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.head(10))\n",
    "        print(\"\\nBottom 10 Most Important Features:\")\n",
    "        print(feature_importance.sort_values('importance', ascending=True).head(10))\n",
    "    \n",
    "    # Accuracy and confusion matrix\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix Heatmap')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4081d1-79e5-4593-a959-3b6e74050171",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Determining a good cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc45c7-3e16-45de-bdad-09915bcb3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_selector.fit(X_train, y_train)\n",
    "\n",
    "importances = pre_selector.feature_importances_\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': importances\n",
    "})\n",
    "feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Distribution plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(feature_imp['importance'], bins=50)\n",
    "plt.title('Distribution of Feature Importance Scores')\n",
    "plt.xlabel('Importance Score')\n",
    "\n",
    "# Cumulative importance plot\n",
    "plt.subplot(1, 2, 2)\n",
    "cumulative_importance = np.cumsum(feature_imp['importance'])\n",
    "plt.plot(range(len(cumulative_importance)), cumulative_importance)\n",
    "plt.title('Cumulative Feature Importance')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Cumulative Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Elbow plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(feature_imp['importance'])), feature_imp['importance'])\n",
    "plt.title('Feature Importance Scores (Elbow Plot)')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()\n",
    "\n",
    "# summary statistics\n",
    "print(\"\\nImportance Score Statistics:\")\n",
    "print(feature_imp['importance'].describe())\n",
    "\n",
    "# potential cutoff points\n",
    "mean_importance = feature_imp['importance'].mean()\n",
    "median_importance = feature_imp['importance'].median()\n",
    "percentile_75 = feature_imp['importance'].quantile(0.75)\n",
    "\n",
    "print(\"\\nPotential Cutoff Points:\")\n",
    "print(f\"Mean importance: {mean_importance:.6f}\")\n",
    "print(f\"Median importance: {median_importance:.6f}\")\n",
    "print(f\"75th percentile: {percentile_75:.6f}\")\n",
    "\n",
    "# features above different thresholds\n",
    "print(\"\\nNumber of features above thresholds:\")\n",
    "thresholds = [mean_importance, median_importance, percentile_75]\n",
    "for threshold in thresholds:\n",
    "    n_features = sum(feature_imp['importance'] > threshold)\n",
    "    print(f\"Threshold {threshold:.6f}: {n_features} features\")\n",
    "\n",
    "# top features and their importance scores\n",
    "print(\"\\nTop 20 features and their importance scores:\")\n",
    "print(feature_imp.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1925d-61a7-4d00-ae7e-2476bf1ae678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 features to see where importance drops significantly\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(100), feature_imp['importance'][:100])\n",
    "plt.title('Top 100 Feature Importance Scores')\n",
    "plt.xlabel('Feature Rank')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.show()\n",
    "\n",
    "# top 20 importance values to see the pattern\n",
    "print(\"\\nTop 20 importance values:\")\n",
    "print(feature_imp['importance'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12323f-94de-4873-b846-fe4dbcce1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features needed for 80% of importance\n",
    "cumsum = np.cumsum(feature_imp['importance'])\n",
    "n_features_80 = len(cumsum[cumsum <= 0.8])\n",
    "print(f\"Features needed for 80% importance: {n_features_80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8266ab-40d3-4c99-b966-1fa9926c058f",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6bd29-7b1a-4a23-b0a6-85f73cbc33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_baseline = model_eval_metrics(X, y, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16581c1a-a701-41ff-afb2-b496dc25c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select = model_eval_metrics(X, y, logreg, select=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e74856-82c4-4e14-bb42-d5ae8762f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select_scale = model_eval_metrics(X, y, logreg, select=True, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01937666-850a-48a2-a16b-fe5bd118a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! be sure to load in the p_cohort that has the new SPOKE mappings and make this the new X and y\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select_scale_spoke = model_eval_metrics(X, y, logreg, select=True, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b92feca-778d-428a-9a3c-9ac24273bae4",
   "metadata": {},
   "source": [
    "**SPOKE Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c848b1f5-63b1-410e-a20b-002ccbbb63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_psev_matrices(directory):\n",
    "    # Get all files that start with 'filtered_patient_psevs_' but don't include 'columns'\n",
    "    psev_files = [f for f in os.listdir(directory) \n",
    "                  if f.startswith('filtered_patient_psevs_') \n",
    "                  and 'columns' not in f\n",
    "                  and f != 'person_id_index.npy']\n",
    "    \n",
    "    matrices = []\n",
    "    column_names = []\n",
    "    \n",
    "    print(\"Loading matrices:\")\n",
    "    for psev_file in sorted(psev_files):\n",
    "        matrix = np.load(os.path.join(directory, psev_file))\n",
    "        \n",
    "        # Load corresponding column names\n",
    "        col_file = psev_file.replace('filtered_patient_psevs_', \n",
    "                                   'filtered_patient_psevs_columns_')\n",
    "        cols = np.load(os.path.join(directory, col_file), allow_pickle=True)  # Added allow_pickle=True\n",
    "        \n",
    "        print(f\"{psev_file}: shape {matrix.shape}, {len(cols)} columns\")\n",
    "        \n",
    "        matrices.append(matrix)\n",
    "        column_names.extend(cols)\n",
    "    \n",
    "    # concatenate horizontally\n",
    "    combined_matrix = np.hstack(matrices)\n",
    "    \n",
    "    # load person ids\n",
    "    person_ids = np.load(os.path.join(directory, 'person_id_index.npy'), allow_pickle=True)  # Added allow_pickle=True\n",
    "    \n",
    "    print(f\"\\nFinal matrix shape: {combined_matrix.shape}\")\n",
    "    print(f\"Number of columns: {len(column_names)}\")\n",
    "    print(f\"Number of patients: {len(person_ids)}\")\n",
    "    \n",
    "    return combined_matrix, column_names, person_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369704ac-9de8-4cf8-a526-3507811019e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'data/psev/p5'\n",
    "psev_mat, psev_cols, psev_pats = load_psev_matrices(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823b673-3357-4d82-8461-267d5e633aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! be sure to load in the p_cohort that has the new SPOKE mappings and make this the new X and y\n",
    "\n",
    "logreg = LogisticRegression(class_weight='balanced', random_state=42, n_jobs=40, solver='saga')\n",
    "y_select_scale_spoke = model_eval_metrics(psev_mat, y, logreg, select=True, scale=True, psev_in=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ce059-5b43-408b-8d83-40aa42c6ef81",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7b5a0-23e8-4fe4-b086-774406bc68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_95_ci(data):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    n = len(data)\n",
    "    \n",
    "    # For 95% CI, we use 1.96 as the critical value\n",
    "    margin_of_error = 1.96 * (std / np.sqrt(n))\n",
    "    \n",
    "    ci_lower = mean - margin_of_error\n",
    "    ci_upper = mean + margin_of_error\n",
    "    \n",
    "    return (ci_lower, ci_upper)\n",
    "\n",
    "\n",
    "# ci = calculate_95_ci(roc_scores)\n",
    "# print(f\"95% CI: ({ci[0]:.4f}, {ci[1]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601ef02-98d6-4b6c-9d7d-d936d4dc46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a figure of all the AUC ROCs\n",
    "\n",
    "plt.figure(figsize=(6, 4), dpi=300)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "\n",
    "\n",
    "#! Below are all the lines for different y_pred_proba for each model\n",
    "\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, no_select_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#4B8BCB', lw=2.5,  \n",
    "#          label=f'All CUIs (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, no_scale_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#EED91F', lw=2.5,  \n",
    "#          label=f'Select CUIs (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, no_spoke_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#FF8C42', lw=2.5,  \n",
    "#          label=f'SPOKE CUIs (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# # Calculate ROC curve\n",
    "# fpr, tpr, _ = roc_curve(y_test, psev_y_pred_proba)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# # Plot ROC curve with more prominent colors\n",
    "# plt.plot(fpr, tpr, color='#FF3C38', lw=2.5,  \n",
    "#          label=f'PSEV (AUC = {np.mean(roc_scores):.4f})')\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='#7B7D7D', linestyle='--', lw=1.5,  \n",
    "         label='Random (AUC = 0.5)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=14, labelpad=10)\n",
    "plt.ylabel('True Positive Rate', fontsize=14, labelpad=10)\n",
    "plt.title('Model Performance: ROC Curve', fontsize=16, pad=20)\n",
    "plt.legend(loc='lower right', fontsize=12, framealpha=0.9)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omop-query",
   "language": "python",
   "name": "omop-query"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
