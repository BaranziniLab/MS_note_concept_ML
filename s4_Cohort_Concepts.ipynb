{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15163d55-be53-4629-95a2-afe4a281823d",
   "metadata": {},
   "source": [
    "# Obtaining Concepts for All Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5c1ad-67e9-4ccb-b9e3-85979879a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# timeoutput\n",
    "import datetime\n",
    "\n",
    "# regex\n",
    "import re\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter \n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a04368-2865-467d-9a3a-df0136646e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the base_path to the IC data location in Wynton\n",
    "\n",
    "\n",
    "# Functions for easy pulling of CDW data\n",
    "\n",
    "def file_path_parquet(filename, datatype):\n",
    "    base_path = f\"path/to/ic/data/{datatype}/\"\n",
    "    parquet_wild = \"/*.parquet\"\n",
    "    return f\"{base_path}{filename}{parquet_wild}\"\n",
    "\n",
    "def rtime():\n",
    "    # Get the current datetime\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    # Define a mapping of days of the week to colors\n",
    "    day_color_mapping = {\n",
    "        0: 'red',       # Monday\n",
    "        1: 'orange',    # Tuesday\n",
    "        2: 'green',     # Wednesday\n",
    "        3: 'blue',      # Thursday\n",
    "        4: 'purple',    # Friday\n",
    "        5: 'brown',     # Saturday\n",
    "        6: 'gray',      # Sunday\n",
    "    }\n",
    "\n",
    "    # Get the day of the week (0=Monday, 1=Tuesday, ..., 6=Sunday)\n",
    "    day_of_week = current_datetime.weekday()\n",
    "    # Get the color based on the day of the week\n",
    "    text_color = day_color_mapping.get(day_of_week, 'black')  # Default to black if the day is not found in the mapping\n",
    "    # Format the current datetime\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    # Generate the formatted output with the corresponding color\n",
    "    formatted_output = f\"\\n<b><span style='color:{text_color}'>Ran: {formatted_datetime}</span></b>\\n\"\n",
    "    # Display the formatted output using Markdown\n",
    "    display(Markdown(formatted_output))\n",
    "    \n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0900ff4-5834-489d-92ba-9f007376b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! change the path to scratch and the username\n",
    "\n",
    "\n",
    "# wynton_username with your actual Wynton username\n",
    "username = 'name'\n",
    "\n",
    "# Spill data that doesn't fit into memory into Wynton Scratch storage (BeeGFS)\n",
    "# Increase up to 12 threads and 150 GB of memory to not overwhelm the system\n",
    "# Recommendation: ~12 GB of memory for each thread\n",
    "# reduce if there are other system limitations in place\n",
    "config_query = f\"\"\"\n",
    "    SET temp_directory = 'path/to/scratch/{username}/duckdb_dir';\n",
    "    SET preserve_insertion_order = false;\n",
    "    SET memory_limit = '150GB';\n",
    "    SET threads TO 12;\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection with configurations\n",
    "con = duckdb.connect()\n",
    "con_info = con.execute(config_query)  # Apply configuration settings\n",
    "\n",
    "display(con_info)\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de93d48-18e6-4225-b667-5360f672a6a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db71035-54fe-40b2-94c9-a91b4b6c1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_csv(\"matched25_cohort.csv\")\n",
    "\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004deabd-eb87-457d-8150-46d8a6f06f0d",
   "metadata": {},
   "source": [
    "### CDW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44a66b-fc4f-4cea-ac0e-7094d5aaa503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deid_note_key and negation terms\n",
    "note_concepts = con.read_parquet(file_path_parquet('note_concepts', 'DEID_CDW'))\n",
    "\n",
    "# linker to patientdurablekey, encoutnerkey, and deid_note_key\n",
    "note_metadata = con.read_parquet(file_path_parquet('note_metadata', 'DEID_CDW'))\n",
    "\n",
    "# note text - only deid_note_key and note_text\n",
    "note_text = con.read_parquet(file_path_parquet('note_text', 'DEID_CDW'))\n",
    "\n",
    "# diagnosis event fact\n",
    "diag_fact = con.read_parquet(file_path_parquet('diagnosiseventfact', 'DEID_CDW'))\n",
    "\n",
    "# patdurabledim\n",
    "patdurabledim = con.read_parquet(file_path_parquet('patdurabledim', 'DEID_CDW'))\n",
    "\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24b06d-dc09-49f0-ab11-42bafbfc9be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Queries\n",
    "\n",
    "Only run this once and save the output! It takes a while to complete and the data can be reused. \n",
    "\n",
    "Skip this section in subsequent runs and proceed to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33906f-7a55-4dea-891d-4ced35ad571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table with the patient IDs\n",
    "temp_df = pd.DataFrame(cohort['patientepicid'].unique(), columns=['patientepicid'])\n",
    "con.register('temp_patients', temp_df)\n",
    "\n",
    "# drop the existing table if exists\n",
    "con.query(\"DROP TABLE IF EXISTS note_key_table\")\n",
    "\n",
    "query_note_key_table = \"\"\"\n",
    "CREATE TABLE note_key_table AS\n",
    "    SELECT patientepicid,\n",
    "        encounterkey,\n",
    "        deid_note_key,\n",
    "        deid_service_date\n",
    "    FROM note_metadata n\n",
    "    WHERE deid_service_date >= DATE '1930-01-01'\n",
    "        AND deid_service_date <= DATE '2027-01-01'\n",
    "        AND EXISTS (\n",
    "            SELECT 1 \n",
    "            FROM temp_patients t \n",
    "            WHERE t.patientepicid = n.patientepicid\n",
    "        )\n",
    "\"\"\"\n",
    "\n",
    "con.query(query_note_key_table)\n",
    "\n",
    "# index the new table\n",
    "con.query(\"CREATE INDEX idx_note_key ON note_key_table(deid_note_key)\")\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ced75-e3a1-4e69-9d66-41e6c9976ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sanity check to be sure everything is all there\n",
    "# tmp = con.query(\"SELECT * FROM note_key_table\").df()\n",
    "# np.sum(tmp['deid_service_date'].isna()) / tmp.shape[0]\n",
    "# del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a52c73-acc7-4804-9b18-9e1a6873b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.query(\"SELECT COUNT(DISTINCT(deid_note_key)) AS count FROM note_key_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13922079-0116-4716-8de8-4e5ec025ab7a",
   "metadata": {},
   "source": [
    "**Save Intermediate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719ad01-1d6a-4931-9c09-51995e046ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_query = \"\"\"\n",
    "SELECT *\n",
    "FROM note_key_table\n",
    "\"\"\"\n",
    "\n",
    "con.execute(f\"COPY ({parquet_query}) TO 'cohort_note_key_table.parquet' (FORMAT PARQUET)\")\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc1b82-ad8a-45ed-bd4c-e802cc229c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_allnote_dates = f\"\"\"\n",
    "SELECT tbl.patientepicid,\n",
    "    tbl.encounterkey,\n",
    "    tbl.deid_note_key,\n",
    "    tbl.deid_service_date,\n",
    "    con.canon_text,\n",
    "    con.vocab,\n",
    "    con.vocab_term_id,\n",
    "    con.cui,\n",
    "    con.negated,\n",
    "    con.history,\n",
    "    con.family_history\n",
    "FROM note_key_table tbl\n",
    "JOIN note_concepts con\n",
    "    ON tbl.deid_note_key = con.deid_note_key\n",
    "\"\"\"\n",
    "\n",
    "note_all_result = con.query(query_allnote_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12f2e9-a7d1-40b1-b68c-f841f16dda8e",
   "metadata": {},
   "source": [
    "**Save Complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b1576-c028-4a8d-91f1-d1e8cdac319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_query = \"\"\"\n",
    "SELECT *\n",
    "FROM note_all_result\n",
    "\"\"\"\n",
    "\n",
    "con.execute(f\"COPY ({parquet_query}) TO 'cohort_note_concepts.parquet' (FORMAT PARQUET)\")\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b776a-ec64-40b0-bce5-a409d2a3fa97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Reformat concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fe145-7906-4b66-887b-95923191b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all concepts within the cohort\n",
    "cohort_con = con.read_parquet(\"cohort_note_concepts.parquet\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65771b-65b4-4d6f-b19e-ec67b1fbf4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all note keys and linkers\n",
    "cohort_note_key = con.read_parquet(\"cohort_note_key_table.parquet\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25123ebe-1ef2-45fe-b737-f37f8e6cdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_con.shape\n",
    "# ~ 94M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6335976-1747-4204-88df-af90052abca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_note_key.shape\n",
    "# ~ 2.4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d31ebe-7b50-4abc-9602-8513b6508f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some patients might have been dropped due to there being no extracted concepts in the notes\n",
    "# keep track of these\n",
    "diff_pats = set(cohort['patientepicid']).difference(set(cohort_con['patientepicid']))\n",
    "len(diff_pats)\n",
    "\n",
    "diff_pats_df = pd.DataFrame(list(diff_pats), columns=['non-matching_pats'])\n",
    "\n",
    "diff_pats_df.to_csv('data/cohort/non-matching_pats.csv', index=False)\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce7504-1541-4442-8ec9-d5eb08704001",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Optional - Check PSM again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee4eef-5706-4660-bc72-7fb2f2315095",
   "metadata": {},
   "source": [
    "Look at \"what if we pulled these patients from the pool directly, is it still good?\" given we lost some patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8db0c-3ff2-4acc-8c57-2e8325d3ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched = pd.read_parquet('data/unmatched_cohort_250319.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496a9cd-e078-49f2-adea-1f5cc18a7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1c145-ba4a-4ca0-b94b-7d889c002f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_prop = unmatched[unmatched['patientepicid'].isin(cohort['patientepicid'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb13fb-fb0e-4f6a-bcd4-91d17eacdd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_prop_diff = cohort_prop[~cohort_prop['patientepicid'].isin(diff_pats)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3da653-029e-4bae-afc5-4b0679e98df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_prop_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d440c4-718a-483f-832e-3dedb2bc5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "def summary_stats_table(data, covariates, categorical, treatment_col):\n",
    "    \n",
    "    def round4(num):\n",
    "        round(num, 3)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    # numeric covariates\n",
    "    for covariate in covariates:\n",
    "        mean_treated = data[data[treatment_col] == 1][covariate].mean()\n",
    "        std_treated = data[data[treatment_col] == 1][covariate].std()\n",
    "        mean_control = data[data[treatment_col] == 0][covariate].mean()\n",
    "        std_control = data[data[treatment_col] == 0][covariate].std()\n",
    "\n",
    "        smd = calculate_smd(data[data[treatment_col] == 1], \n",
    "                            data[data[treatment_col] == 0], \n",
    "                            [covariate])[covariate]\n",
    "\n",
    "        rows.append({\n",
    "            'Covariate': covariate,\n",
    "            'Mean_Treated': mean_treated,\n",
    "            'Std_Treated': std_treated,\n",
    "            'Mean_Control': mean_control,\n",
    "            'Std_Control': std_control,\n",
    "            'SMD': smd\n",
    "        })\n",
    "    \n",
    "    # categorical covariates\n",
    "    for covariate in categorical:\n",
    "        prop_treated = data[data[treatment_col] == 1][covariate].value_counts(normalize=True)\n",
    "        prop_control = data[data[treatment_col] == 0][covariate].value_counts(normalize=True)\n",
    "        \n",
    "        smds = calculate_smd_categorical(prop_treated, prop_control, covariate)\n",
    "        \n",
    "        for category in prop_treated.index:\n",
    "            rows.append({\n",
    "                'Covariate': f\"{covariate}_{category}\",\n",
    "                'Mean_Treated': prop_treated[category] * 100,\n",
    "                'Std_Treated': np.nan,  # Standard deviation is not applicable for proportions\n",
    "                'Mean_Control': prop_control[category] * 100,\n",
    "                'Std_Control': np.nan,  # Standard deviation is not applicable for proportions\n",
    "                'SMD': smds[category]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def calculate_smd(group1, group2, var_list):\n",
    "    smds = {}\n",
    "    for var in var_list:\n",
    "        mean1 = group1[var].mean()\n",
    "        mean2 = group2[var].mean()\n",
    "        std1 = group1[var].std()\n",
    "        std2 = group2[var].std()\n",
    "\n",
    "        smd = abs(mean1 - mean2) / np.sqrt((std1**2 + std2**2) / 2)\n",
    "        smds[var] = smd\n",
    "    return smds\n",
    "\n",
    "\n",
    "def calculate_smd_categorical(prop_treated, prop_control, covariate):\n",
    "    smds = {}\n",
    "    for category in prop_treated.index:\n",
    "        prop_treated_val = prop_treated.get(category, 0)\n",
    "        prop_control_val = prop_control.get(category, 0)\n",
    "        \n",
    "        smd = abs(prop_treated_val - prop_control_val) / np.sqrt((prop_treated_val * (1 - prop_treated_val) + prop_control_val * (1 - prop_control_val)) / 2)\n",
    "        smds[category] = smd\n",
    "    return smds\n",
    "\n",
    "\n",
    "def plot_covariate_balance(data, covariates, treatment_col):\n",
    "    for covariate in covariates:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.kdeplot(data[data[treatment_col] == 1][covariate], label='Treated')\n",
    "        sns.kdeplot(data[data[treatment_col] == 0][covariate], label='Control')\n",
    "        plt.title(f'Distribution of {covariate} by Treatment Status')\n",
    "        plt.xlabel(covariate)\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c3564-ff10-4d26-a250-86531de58383",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = ['follow_up', 'n_notes', 'age_at_first_visit']\n",
    "categorical = ['gender_concept_id', 'race_concept_id']   # numerics that should be treated as categoricals\n",
    "summary_stats = summary_stats_table(cohort_prop_diff, covariates, categorical, 'is_ms')\n",
    "np.round(summary_stats, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bafe0-f035-4a39-89c2-2a24c06db20e",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Ok, we still good**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ca13fb-f03f-4019-b163-bf60de2762a2",
   "metadata": {},
   "source": [
    "### Continue\n",
    "\n",
    "The continuation of the script filters out dropped patients and then creates three separate cohorts based on their preexposure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda22a6d-6a28-421a-9917-2ddb3ec153be",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_pats = pd.read_csv('data/cohort/non-matching_pats.csv')\n",
    "isms_key = cohort_t[['patientepicid', 'is_ms']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bfeb9-1928-49be-b7ed-ca8ab8aabdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine cohort here\n",
    "cohort = cohort[~cohort['patientepicid'].isin(diff_pats['non-matching_pats'])]\n",
    "# add time in\n",
    "pat_note_times = cohort_note_key[['patientepicid', 'deid_service_date']].groupby('patientepicid').min().reset_index()\n",
    "cohort_t = pd.merge(cohort, pat_note_times, how='inner')\n",
    "\n",
    "rtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df1c95-d854-4307-a174-d44493a5bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some patients might be lost\n",
    "sum(cohort_t['is_ms']==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67169c-f797-403e-b7fa-4310f6f943de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(x):\n",
    "    if 1 <= x < 3:\n",
    "        return 1\n",
    "    elif 3 <= x < 5:\n",
    "        return 3\n",
    "    elif x >= 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "cohort_t['exposure_group'] = cohort_t['follow_up'].apply(create_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f60e5c-9c4a-4e88-9e4d-6387d6394317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the exposure periods [p1, p3, p5]\n",
    "def create_exposure_periods(df):\n",
    "    for years in [1, 3, 5]:\n",
    "        df[f'p{years}'] = (df['exposure_group'] >= years).astype(int)\n",
    "        df[f'p{years}_exposure_end'] = df.apply(\n",
    "            lambda row: row['deid_service_date'] + relativedelta(years=years) \n",
    "            if row['exposure_group'] >= years else np.nan, \n",
    "            axis=1\n",
    "        )\n",
    "    return df\n",
    "\n",
    "cohort_t = create_exposure_periods(cohort_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2057a-b50e-4fc5-9250-5c0144f89059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_note_keys(note_keys, pats, period):\n",
    "    if 'deid_service_date' in pats.columns:\n",
    "        pats.drop(columns=['deid_service_date'], inplace=True)\n",
    "    pat_subset = pats[pats[period] == 1]\n",
    "    df_merge = pd.merge(note_keys, pat_subset, on='patientepicid', how='inner')\n",
    "    df_merge = df_merge[df_merge['deid_service_date'] < df_merge[f'{period}_exposure_end']]\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebeab5-d3a4-4e4d-8b24-43738f656282",
   "metadata": {},
   "source": [
    "**Separate out to three different cohort processes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbd3f2-2d0d-4d49-b1bf-c9c64b16f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# reverses the dictionary\n",
    "def reverse_dict(original):\n",
    "    reversed_dict = {}\n",
    "    for key, values in original.items():\n",
    "        for value in values:\n",
    "            reversed_dict[value] = key\n",
    "    return reversed_dict\n",
    "\n",
    "# combines columns and rename them based on the mapping\n",
    "def combine_columns(df, mapping):\n",
    "    # filter to only keep columns that are in the mapping\n",
    "    valid_columns = [col for col in df.columns if col in mapping]\n",
    "    df_filtered = df[valid_columns]\n",
    "    \n",
    "    print(f\"Total columns in original dataframe: {len(df.columns)}\")\n",
    "    print(f\"Columns found in mapping: {len(valid_columns)}\")\n",
    "    \n",
    "    # group columns by their mapped values\n",
    "    column_groups = defaultdict(list)\n",
    "    for col in df_filtered.columns:\n",
    "        column_groups[mapping[col]].append(col)\n",
    "        \n",
    "    # create all combined columns at once\n",
    "    combined_cols = {\n",
    "        new_col: df_filtered[old_cols].sum(axis=1) \n",
    "        for new_col, old_cols in column_groups.items()\n",
    "    }\n",
    "    new_df = pd.DataFrame(combined_cols)\n",
    "    \n",
    "    print(f\"\\nCombined {len(df_filtered.columns)} columns to {len(new_df.columns)}\")\n",
    "    return new_df\n",
    "\n",
    "# only normalize rows that are non-zero\n",
    "# you will get rows that sum to 0 since a lot of the CUI information is lost when moving into SPOKE\n",
    "def normalize_non_zero_rows(df):\n",
    "    df_norm = df.astype(float)\n",
    "    \n",
    "    row_sums = df_norm.sum(axis=1)\n",
    "    \n",
    "    non_zero_mask = row_sums != 0\n",
    "    df_norm.loc[non_zero_mask] = df_norm.loc[non_zero_mask].div(row_sums[non_zero_mask], axis=0)\n",
    "    \n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Rows with zero sums: {(~non_zero_mask).sum()}\")\n",
    "    print(f\"Rows normalized: {non_zero_mask.sum()}\")\n",
    "    \n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bccab-24aa-4636-b4fd-a6c8ce43a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you get this data from the SPOKE embedding overlap with CUIs using BioPortal\n",
    "# or saved for the purpose of this cohort\n",
    "# Load dictionary from a JSON file\n",
    "with open('data/mapping/spoke_mappings.json', 'r') as json_file:\n",
    "    spoke_mappings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f600d-b5c4-4aa0-a5f3-f478cd9aedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the dictionary\n",
    "spoke_mappings_rev = reverse_dict(spoke_mappings)\n",
    "print(\"Reversed spoke mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74131f8-e8bf-4c43-9f36-8610a75b11df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_concepts(valid_note_keys, cohort_note_key, cohort_con, isms_key, periods=[], spoke_mappings_rev=None):\n",
    "    \n",
    "    if len(periods)==0:\n",
    "        ValueError(\"Must provide some periods to iterate across\")\n",
    "\n",
    "    for p in periods:\n",
    "        p_notes = valid_note_keys(cohort_note_key, cohort_t, p)\n",
    "        print(f'period {p}:', p_notes.shape)\n",
    "\n",
    "        p_ms_con = cohort_con[cohort_con['deid_note_key'].isin(p_notes['deid_note_key'])]\n",
    "        print(f'period {p} concepts:', p_ms_con.shape)\n",
    "\n",
    "        max_cui_vocab = set(p_ms_con['cui'])\n",
    "        print(\"Max number of unique CUI concepts:\", len(max_cui_vocab))\n",
    "\n",
    "        pats_lost = len(set(cohort_t['patientepicid'])) - len(set(p_ms_con['patientepicid']))\n",
    "        print(f\"{pats_lost} period {p} patients were lost due to note density and follow_up time differences\")\n",
    "\n",
    "        pivot_p = pd.pivot_table(\n",
    "            p_ms_con,\n",
    "            values='deid_note_key',\n",
    "            index='patientepicid',\n",
    "            columns='cui',\n",
    "            aggfunc='count',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        if spoke_mappings_rev is not None:\n",
    "            pivot_p = combine_columns(pivot_p, spoke_mappings_rev)\n",
    "\n",
    "        # Normalize by dividing each row by the number of concepts for that patient\n",
    "        pivot_p = normalize_non_zero_rows(pivot_p)\n",
    "        pivot_p = pd.merge(pivot_p, isms_key, left_index=True, right_on='patientepicid')\n",
    "        print(\"Patinets:\", sum(pivot_p['is_ms']))\n",
    "        \n",
    "        if spoke_mappings_rev is not None:\n",
    "            pivot_p_out.to_parquet(f\"{p}_cohort_spoke.parquet\")\n",
    "        else:\n",
    "            pivot_p_out.to_parquet(f\"{p}_cohort.parquet\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1238d-5da4-4500-8dc8-00ebd6c2aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can take a while\n",
    "pivot_concepts(valid_note_keys, cohort_note_key, cohort_con, \n",
    "               isms_key, periods=[\"p1\", \"p3\", \"p5\"], \n",
    "               spoke_mappings_rev=spoke_mappings_rev)\n",
    "\n",
    "rtime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omop-query",
   "language": "python",
   "name": "omop-query"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
